---
title: 'Random Walks I'
date: 2024-05-13
permalink: /posts/2012/08/blog-post-1/
tags:
  - probability 
  - maths
---

This will be my first blog post in an endeavour to write regular bite-sized articles that improve my understanding of technical topics and ability to communicate them. I intend the first few articles to focus on mathematical problems I have found interesting or enlightening, particularly in probability and stochastic processes, areas that I am familiar with from honours and my time as a quant. 

Let's begin with a classic problem on random walks:
> Let \\(a\\), \\(b\\) be positive integers. What is the probability that a symmetric random walk starting at the origin reaches \\(b\\) before \\(-a\\)?

Recursive Solution
======
Let's consider the equivalent problem of a random walk starting at \\(a\\). We want the probability that the ranodm walk reaches \\(a+b\\) before \\(0\\). 

Let \\(p_{k}\\), \\(k=0,1\ldots, a+b\\), be the probabilities that a random walk starting at \\(k\\) reaches \\(a+b\\) before \\(0\\). Clearly \\(p_0=0\\) and \\(p_{a+b}=1\\). For \\(k=1,\ldots, a+b-1\\), 
\\[p_{k} = \frac{p_{k-1} + p_{k+1}}{2} \iff p_{k+1} = 2p_{k} - p_{k-1}.\\]
Writing out the first few terms, a pattern emerges:

$$
  \begin{align*}
    p_2 &= 2p_1 \\
    p_3 & = 2p_2 - p_1 = 3p_1 \\
    p_4 & = 2p_3 - p_2 = 4p_1
    \end{align*}
$$

A simple induction yields \\(p_k = k p_1\\) for \\(k=0,1,2,\ldots a+b\\), and combining with the boundary condition \\(p_{a+b} = 1\\) gives \\(p_{k}=\frac{k}{a+b}\\). The solution to the problem is then \\(p_a = \frac{a}{a+b}\\). 

Of course, there are many other ways we could have solved the recurrence relation - for example, the method of characteristics for linear second-order difference equations, or linear algebra.

Stochastic Solution
======
This solution requires some machinery from stochastic processes which I will not discuss here. The magic ingredient is the Optional Stopping Theorem, which essentially says that the expected value of stopped martingale equal to its initial value. I was not able to appreciate how useful this theorem is until many years after seeing it in undergrad (sorry, professors).
> **Optional Sampling Theorem**: Let \\(\\{M_n\\}_{n=0}^{\infty}\\) be a martingale and \\(\tau\\) be an almost surely finite stopping time. Then, 
> $$
>  E[M_\tau] = E[M_0].
> $$

Let \\(X:=\\{X_n\\}_{n=0}^{\infty}\\) be a symmetric random walk starting at the origin. Then we can write

$$
  X_n = \sum_{k=1}^{n} Z_n, 
          \quad \text{where } 
          Z_n =\begin{cases}
                +1 & \quad \text{w.p. } \frac{1}{2}\\
                -1 & \quad \text{w.p. } \frac{1}{2}\\
                \end{cases}\quad \text{and are independent}.
$$

Let \\(\tau\\) be the first time \\(X\\) hits either \\(-a\\) or \\(b\\), and \\(A, B\\) be the events that the random walk hits \\(-a\\) or \\(b\\) first respectively. Then, assuming \\(\tau\\) is finite, \\(\Pr(A) + \Pr(B) = 1\\). Applying the Optional Stopping Theorem,

$$
  \begin{align*}
     E[X_{\tau}] = X_0 & = 0.\\[1em]
     -a\Pr(A) + b \Pr(B) & = 0.
    \end{align*}
$$

Solving the linear system from the two equations yields
$$
  \Pr(A) = \frac{b}{a+b}, \quad \Pr(B) = \frac{a}{a+b}.
$$

Of course, this all hinges on the assumption that \\(\tau\\) is finite, which we now prove.

Proof that \\(\tau\\) is finite
------
It suffices to prove that \\(E[\tau]<\infty\\).

Let \\(E_{i}\\), \\(i=0,1,\ldots\\) be the event that \\(Z_{(a+b)i + 1}, Z_{(a+b)i+2},\ldots Z_{(a+b)(i+1)}\\) are all equal to \\(1\\). Let \\(n\\) be a positive integer. If any of the events \\(Z_i\\) occur for \\(i\leq n\\), then the random walk must be outside the interval \\((-a, b)\\). Thus,

$$
  \begin{align}
    \\{\tau > (a+b)n \\} & \subseteq \cap_{i=0}^{n-1} E_{i}^{\complement} \\[1em]
    \Pr(\tau > (a+b)n) &\leq \Pr\left(\cap_{i=0}^{n-1} E_{i}^{\complement} \right) \\[1em]
    & = \prod_{i=0}^{n-1} (1 - \Pr(E_i)) \quad E_i\text{ are independent}\\
    & = (1 - 2^{-(a+b)})^{n}
    \end{align}
$$

Finally, since \\(\tau\\) is positive random variable,

$$
  \begin{align}
    E[\tau] & = \sum_{n=0}^{\infty} \Pr(\tau > n) \\
            & = \sum_{m=0}^{\infty}\sum_{n=0}^{a+b-1} \Pr(\tau > (a+b)m + n) \\
            & \leq (a+b)\sum_{m=0}^{\infty} \Pr(\tau > (a+b)m) \\
            & \leq (a+b) \sum_{m=0}^{\infty} \left(1 - 2^{-(a+b)}\right)^{m}\\
            & <\infty.
    \end{align}
$$

One can prove the following for positive discrete random variables as exercises
1. \\(E[X] <\infty \implies P(X=+\infty) = 0\\)
2. \\(E[X]=\sum_{n=0}^{\infty}\Pr(X > n)\\)
